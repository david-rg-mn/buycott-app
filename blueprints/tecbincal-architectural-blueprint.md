# Buycott System Architecture Blueprint (Phases 0–2)

## 1. System Identity and Governance Layer

**Core Definition:** Buycott is fundamentally defined as a *“semantic geographic capability index”* – a neutral mapping of where specific item capabilities can be found locally. It is **not** a recommendation engine, advertising platform, or popularity-based directory. This immutable identity governs all design decisions, influencing the database schema, search logic, ontology scope, and UI presentation. By locking the system’s identity in this way, the architecture enforces permanent neutrality and resists drift toward profit-driven ranking or ads.

 **Discovery Model:** The platform answers the question *“Where can this item likely be obtained nearby?”* and explicitly avoids questions of quality or preference. Results are presented based on *semantic capability proximity* (i.e. how well a business can fulfill the queried item) rather than any notion of “best” or “most popular”. This governance ensures that **search results are neutral** – every design element avoids implicit recommendations or bias, aligning with Buycott’s civic mission.

 **Guiding Principles:** From the outset, core civic design principles are baked into the governance layer. These include **Neutrality** (no ranking by profit or popularity – evidence is shown for transparency only, not to influence order), **Civic Alignment** (no ads or sponsorship; open-source and donation-supported ethos), and **Transparency** (all data is traceable to public sources; semantic reasoning is explainable). Together, these principles serve as non-negotiable architectural constraints introduced in Phase 0 and carried through all phases. Any new feature in later phases must be evaluated against these invariants to maintain the system’s civic, neutral character.

## 2. Ontology Design and Governance

**Hierarchical Ontology Structure:** The system’s ontology defines a controlled vocabulary of product types and capability domains in a tree hierarchy. Specific items map up through broader categories to general domains, enabling semantic generalization. For example, an item like “USB-C to Lightning cable” is linked to parent categories like “USB cable” → “charging cable” → “phone accessory” → “electronics accessory”. This structure encapsulates semantic relationships so the system can infer that a store carrying *phone accessories* might satisfy a query for a specific cable, even if not explicitly mentioned. The ontology thus provides the semantic scaffolding for query expansion and matching.

 **Governance Rules:** Phase 0 established strict ontology governance to ensure consistency and neutrality. Allowed relationships are simple parent→child links with a bounded depth (3–5 levels maximum). **Cyclic or graph relationships are disallowed** to keep the hierarchy acyclic and comprehensible. The ontology’s scope is limited to **product/service capabilities only** – it never encodes business preferences or quality (no tags like “best” or “cheap”). Business-specific terms are prohibited, preventing any one store from injecting proprietary jargon into the global ontology. These constraints guard against ontology corruption or semantic drift, so that expansions remain broadly applicable and civically neutral.

 **Schema and Explainability:** Ontology terms are stored in a dedicated table with fields for the term, its parent term, hierarchical depth, and an embedding vector representation. Each term’s vector (e.g. 384-dimensional) places it in the same semantic space as queries and business text, enabling vector-based reasoning about similarity. The ontology database may also track the source or provenance of each term for transparency. This schema ensures the ontology remains **explainable and traceable** – every relationship can be inspected, and expansions can be explained by referencing the ontology hierarchy. Phase 1 implemented this ontology structure in a PostgreSQL table (with a `pgvector` column for embeddings) and initial taxonomy content. By Phase 2, the ontology was leveraged not just for query expansion but also for  *user-facing features* : e.g. semantic search suggestions and capability tags. These enhancements (Phase 2) reuse the governed ontology; for instance, **Ontology Suggestion Integration** traverses the ontology graph to suggest related search terms (e.g. typing “moving supplies” might prompt suggestions like “boxes” or “packing tape” drawn from child terms). The governance model thus not only constrains ontology content but also guides its expansion and usage in the system.

## 3. Data Model and Schema Definitions

**Business Entity Schema:** All business information is structured in a relational schema that reflects the system’s semantic, neutral focus. The core `businesses` table defines each business with fields such as unique `id`, `name`, geographic coordinates (`lat`, `lng`), and a semantic `embedding` vector of its descriptive text. Additional fields capture civic-oriented metadata: `is_chain` (boolean flag if it’s part of a chain) and `chain_name` (name of the chain, if applicable), along with `last_updated` timestamp for data freshness tracking. Notably, there are  **no fields for popularity, clicks, or monetization** ; in fact, Phase 0 explicitly forbids any schema fields like `popularity_score`, `click_count`, `revenue_metric`, etc., to  *structurally prevent ranking manipulation* . The absence of such fields ensures that no part of the backend can even *store* data that might lead to biased ranking – a direct manifestation of the permanent neutrality constraint.

 **Semantic Attribution Tables:** Two auxiliary tables support transparency and explainability of search results. The `business_sources` table logs the origin of each piece of business data (e.g. “website”, “public directory”, plus the URL and fetch timestamp). This allows every capability inference to be traced back to public information, reinforcing trust that the system’s knowledge is civic (publicly available) rather than covert. Similarly, the `business_capabilities` table maps each business to the ontology terms (product types) it is inferred to offer, with a confidence score and reference to supporting evidence. For example, a hardware store might have entries linking it to “paint supplies” or “garden tools” with confidence levels, based on extracted text. This table effectively materializes each business’s  *capability profile* , which Phase 2 leverages for the **Capability Discovery View** (displaying “Likely carries: …” lists in the UI). Storing these in the schema enables fast queries for explaining results and visualizing a store’s semantic range.

 **Additional Data Fields:** By Phase 2, the data model expanded to support new filtering features. For instance, **business hours** are stored (either in the main business table or a linked `business_hours` table) to enable “open now” filtering. This likely includes hours of operation for each day and perhaps a quick “currently_open” flag computed based on `current_time`. The `last_updated` field in businesses, introduced in Phase 0, becomes user-facing in Phase 2 as a “data freshness” indicator (e.g.  *“Last updated: 14 days ago”* ) to signal information recency. The schema’s evolution through Phase 1 and Phase 2 has been minimal and additive – new tables (`business_sources`, `business_capabilities`, ontology terms) and fields (hours, chain flags) were added to enable transparency and filtering, without altering the core structures defined in Phase 0. All data model decisions trace back to Phase 0 constraints: ensure neutrality (no biased fields), ensure traceability (sources), ensure explainability (capabilities), and support the local-first mission (chain info for filtering).

## 4. Semantic Search Pipeline

**Query Processing Flow:** Buycott’s search pipeline is a semantic vector-based process established in Phase 0 and implemented in Phase 1. Every search query undergoes a fixed sequence of steps:

1. **Embedding Generation:** The user’s query text is converted into a high-dimensional embedding vector using a sentence-transformer model (e.g. MiniLM or BGE). This vector represents the query’s meaning in the semantic space.
2. **Ontology Expansion:** The query term is expanded via the ontology to include broader related concepts. For example, a query for “USB-C to Lightning cable” will internally also consider embeddings for parent terms like “USB cable”, “charging cable”, etc., up the ontology hierarchy. This expansion (limited to 3–5 levels up) increases recall by capturing stores that might not mention the exact specific item but stock its general category.
3. **Expanded Embeddings:** Embeddings are generated for each of these related ontology terms as well. Now the query is represented by a set of vectors: the original query and its semantic parents.
4. **Vector Similarity Search:** The system performs a vector similarity search against the precomputed embeddings of all businesses in the database. This is done via the pgVector index in PostgreSQL, finding businesses whose textual content embedding is closest to any of the query vectors. Both the direct query and expanded concepts contribute matches, which are then combined. Critically, matching is purely based on  **embedding similarity and ontology relationships** , never on popularity or sponsored boosts. This ensures the result set is determined by semantic relevance only, in line with Phase 0’s neutrality rule.
5. **Local-First Filtering:** The initial candidate results are filtered by the *“local-first”* policy before presentation. By default, chain businesses are filtered out (`is_chain = false`) unless the user has explicitly toggled to include them. This means the top results favor independent local businesses, reinforcing the civic mission, while still allowing the user the choice to “show chain stores” if desired. Additional Phase 2 filters like “open now” and “within walking distance” may also be applied at this stage if the user enabled them (these check business_hours and distance respectively, see Section 8). All filtering logic remains transparent and user-controlled, never hidden ranking adjustments.
6. **Result Assembly:** The filtered results are then sorted by a permissible criterion – typically **geographic distance** from the user’s location (since two stores equally capable of fulfilling the query should be ordered by nearest first). Other sorts like relevance are essentially handled by the vector similarity step itself, and any attempt to sort by popularity or revenue is disallowed by design. For each result, an evidence score is computed (see Section 6) and the “minutes away” travel time is fetched (see Section 7 and 8) before the result is returned.
7. **Response Output:** Finally, the backend returns the results to the frontend through the API. The response includes the list of matching businesses with details needed for UI: business info, distance/travel time, the evidence strength score, and any relevant flags (e.g. is_chain or capability list). No extraneous recommendation data is included – the output is a neutral candidate set that the frontend will display without re-ranking.

Throughout this pipeline, **explainability** is a key consideration. Each step is logged or can be reproduced for debugging/inspection: e.g. which ontology expansions were used, which sources contributed to a match, etc. Phase 0 mandated that the search process remain *semantic and explainable* – Phase 1 achieved this by keeping the pipeline simple and deterministic (no black-box learning-to-rank), and Phase 2 built on it by adding user-facing explanations (e.g. an “Evidence” view to show why a result was included, see Section 6). The end result is a search pipeline that is robust against bias: it strictly uses semantic content and distance, with all filtering driven by user intent or civic defaults.

## 5. Embedding and Extraction Infrastructure

**Knowledge Extraction (OpenClaw Agents):** Buycott populates its database through a custom crawling/extraction system (code-named  **OpenClaw** ). These agents scour public sources to gather textual data about local businesses. Allowed sources include business websites, public directory listings, and public business descriptions (e.g. Google Places snippets). OpenClaw operates under strict civic constraints – it will **never extract private or proprietary data** like internal inventory, transaction records, or any user behavior data. This ensures the knowledge base remains *civic* (built only from publicly available information) and respects privacy and neutrality. Practically, an OpenClaw agent performs steps such as: find a business’s website or public page, fetch and parse the content, extract relevant text (product mentions, descriptions of services, etc.), and repeat periodically to keep data fresh. Phase 1 implemented an initial batch extraction to seed the database, while Phase 2 emphasized maintaining a `last_updated` timestamp and a continuous refresh mindset (with Phase 3 expected to introduce continuous crawling).

 Each OpenClaw agent runs as a separate process, scalable across a cluster. For performance, the infrastructure can leverage distributed processing (e.g. NVIDIA Spark on DGX for parallel embedding computation). This parallelism allows the system to generate embeddings for hundreds or thousands of businesses in parallel, keeping the search index up to date. The extraction + embedding pipeline is primarily implemented in Python, enabling easy integration of machine learning libraries and web scraping tools.

 **Embedding Generation:** Once business text is extracted, it is converted into a numerical vector using a  **semantic embedding model** . Phase 1 recommended lightweight yet effective sentence transformers (such as `BAAI/bge-small-en-v1.5` or `sentence-transformers/all-MiniLM-L6-v2`) to generate a ~384-dimensional embedding for each text snippet. These models capture semantic similarity: businesses offering similar products will have closer vector representations in this embedding space. Notably, embedding generation is constrained to use **only the extracted public text** as input. No external “popularity” signals or user engagement metrics influence the embeddings – this guarantees what we call **semantic purity** in Phase 0. In other words, a big-box store and a local shop with the same product description should get similar embeddings, rather than the big-box being weighted higher due to brand popularity. All embeddings are computed offline by the extraction pipeline and stored in the database for fast retrieval.

 **Vector Database:** The system uses PostgreSQL with the pgVector extension as its primary data store and vector index. Each business record stores its embedding in a vector column (`VECTOR(384)` for example) which is indexed for approximate nearest-neighbor search. This simplifies the architecture: a single Postgres database can handle both the relational queries (filtering by location, chain status, hours, etc.) and the vector similarity search for semantic matches. It provides transactional updates (e.g. when refreshing a business’s data and embedding) and consistency across data tables. The choice of Postgres aligns with the open-source, self-hostable philosophy of Buycott (no reliance on proprietary vector DB services), and pgVector was chosen in Phase 1 for its integration and adequate performance at MVP scale. As Buycott grows, this could be scaled out or swapped for a dedicated vector search service if needed, but through Phase 2, Postgres satisfied our needs.

 In summary, the extraction and embedding infrastructure is designed to continuously **enrich a semantic knowledge base** of local businesses while adhering to civic data standards. It is a pipeline that starts with public information and ends with high-dimensional vectors in the database. The design decisions (public-only data, no popularity weighting, open-source tooling) all trace back to Phase 0’s governance: keeping the system neutral, explainable, and aligned with public interest.

## 6. Evidence Strength and Transparency Layer

**Evidence Score Computation:** Every search result includes an **Evidence Strength Score** – a numerical indicator (0–100) of how strongly the system believes the business matches the query. This score is primarily a function of the semantic similarity between the query and business embeddings. In Phase 1, the evidence score calculation was defined as the normalized cosine similarity between the query vector and the business’s vector (or the best match among the expanded query vectors). Optional secondary factors can contribute, such as ontology overlap (if the business explicitly mentions a parent category of the query) or explicit keyword matches from the extracted text, but these are minor adjustments. The final score is scaled to a 0–100 range and presented to the user as a colored indicator (e.g. a gradient-filled square on the map pin). Crucially, as per Phase 0’s rules, this evidence score is for  **transparency only – it never affects ranking or filtering** . Whether a score is 99 or 70 does not change the order of results; it is shown simply to communicate confidence. This strict separation ensures that we do not create a feedback loop where high scores get more visibility and hence implicitly become a new “ranking”; all results meeting the query criteria are shown (subject to distance sorting) regardless of score, but the user gains insight into why they were included.

 **Evidence Explanation Feature:** In Phase 2, the system added an interactive explanation layer to further **build user trust** in these semantic matches. Users can tap on the evidence score indicator for any result to open an “Evidence Explanation” panel. This panel lists the key reasons why the business appeared for the query, drawn directly from the data. For example, it might show bullet points such as:  *“Website mentions  **phone accessories** ”* ,  *“Category:  **electronics repair** ”* ,  *“Semantic similarity match:  **charging cable** ”* ,  *“Ontology expansion:  **electronics accessory** ”* . Each point corresponds to an evidence source: the first might come from the business’s website text, the second from a known category/tag in a public directory, the third from the embedding similarity hitting on a related term, and the fourth from the ontology reasoning. These traces make it clear how the system inferred the capability.

 To support this feature, Phase 2 expanded backend data tracking: the semantic search pipeline now retains metadata like which ontology terms matched and the raw similarity scores, and the database stores references to evidence source snippets. For instance, the `business_capabilities` table ties businesses to ontology terms with a confidence score, and an `evidence_sources` structure is maintained for each result to enumerate matching phrases or attributes. All this is exposed via a new API endpoint `/evidence_explanation` that the frontend calls to populate the panel. The outcome is a highly transparent system – users not only see a score but can inspect the *why* behind it, satisfying the Phase 0 goal of explainability and the Phase 2 goal of  **trust-building UX** .

 **Source Transparency and Data Freshness:** Complementing the evidence view, Phase 2 also introduced a **Source Transparency** modal. This simply lists where the business’s information was obtained. For example, it might say “Sources: Business website, Public directory listing, Google Places description” for a given store. This reassures users that the data is from open, reputable sources (and implicitly that Buycott is not relying on user tracking or paid submissions). Implementing this required adding the `business_sources` table (outlined in Phase 0 but now actively used) which tracks each business’s data sources. The frontend can fetch this via an API and display a simple list, reinforcing Buycott’s civic neutrality by showing the absence of any sponsored or biased data inputs. Additionally, each business entry shows a **“Last updated X days ago”** note. This Data Freshness indicator (powered by the `last_updated` field and calculated server-side) addresses reliability concerns. Users can trust that information is reasonably up-to-date, or know when it might be stale. It also implicitly communicates that the system is continuously maintained (and motivates the team to keep the OpenClaw crawlers running!).

 All these elements form an *evidence and transparency layer* that is deeply integrated into the architecture. The scoring algorithm, the storage of evidence metadata, and the user interface all work together to make the system’s semantic inference process visible and trustworthy. This approach stems from Phase 0’s mandate for transparency and Phase 1’s design principle of making evidence visible (rather than using it secretly for ranking).

## 7. Capability Visualization and Discovery UX

**Map-Based Visualization:** The frontend is built around a map-centric experience (inspired by transit maps), which doubles as a capability visualization tool. The **primary UI is a full-screen map** with the user’s location centered. Search results appear as pins on this map. Each result pin carries a visual summarization of key info: a small on-pin display shows the travel time in minutes (e.g. “6m”) and an evidence strength score (e.g. “82”) in a distinct gradient-colored square. This design, implemented in Phase 1, immediately communicates *immediacy* (how close/quickly the item can be obtained) and *confidence* (how likely the item is there). The map pins avoid any ranking numbers or star ratings – every pin is an equal part of the geographic answer, with only factual metrics shown. This spatial presentation inherently emphasizes discovery: users see options around them rather than a textual top-10 list.

 Clicking a pin brings up a **Bottom Sheet detail view** for that business. The bottom sheet (Phase 1 design) displays the business name, address/distance (or minutes away), the evidence strength indicator, and actionable buttons like “Get Directions”, “Call”, “Website”, plus basic info like hours. Importantly, it also lists **source evidence indicators** or capability hints. For example, it might show icons or text tags denoting the sources of match (website, directory) or key matching terms. What it deliberately does *not* show are any rankings, ads, or user reviews. This keeps the focus on “Can this place fulfill your need?” rather than “How good is it?” – consistent with the system’s capability index identity.

 **Capability Discovery View:** Phase 2 added a new panel in the business detail to explicitly list a business’s likely capabilities. This **Capability Profile** shows a bulleted list under a heading like “Likely carries:” followed by items derived from that business’s semantic footprint. For example, a camera shop’s profile might list “camera film, camera batteries, photo printing, camera straps” as inferred items it carries. These are generated by looking at the ontology terms linked to the business (from the `business_capabilities` table) and selecting a few representative ones with high confidence. The backend populates this via the `/business_capabilities` API, which queries the semantic index to find which product categories the business is strongly associated with. This feature turns the raw data in our ontology mapping into a user-facing visualization of the store’s niche. It helps users *discover related capabilities* of a business that they might not have searched for explicitly, and reinforces trust (“this shop is knowledgeable in X domain”). Essentially, it’s surfacing the results of our semantic indexing in a human-readable form.

 **Discovery Highlighting & Badges:** To further encourage exploration, the UI emphasizes certain types of businesses visually. One Phase 2 feature is  **Discovery Highlighting** , where independent or less familiar businesses are given a subtle glow or highlight on the map. This might manifest as a slight halo around pins that represent *new or unique* local spots. The logic for this uses `is_chain = false` (independent) as a criterion, possibly combined with heuristics for “low familiarity” or “specialty” in future. The impact is to draw the user’s eye to local gems they might otherwise overlook, aligning with Buycott’s pro-local stance. In a similar vein, **badges** are used in the interface to communicate special statuses: a *“Specialist”* badge for businesses that focus on a narrow domain, and an *“Independent”* badge for non-chains. A specialist badge might appear on, say, a camera store or an art supply shop – determined by analyzing the business’s ontology profile for a narrow focus (e.g. most capabilities fall under a single sub-tree, indicating a niche). The independent badge simply reflects `is_chain = false` and is shown on details or listings to positively identify local independent businesses. Both badges have an important UX role: they quickly convey the nature of the business (niche expertise or local ownership) which can guide user decision-making in a civic-minded way. They are purely informative and support filtering (e.g. the independent badge reinforces the default local-only filter’s effect, see Section 8).

 **Semantic Query Suggestions:** Buycott’s UX also helps users formulate and expand searches in a semantic manner (Phase 2’s **Semantic UX and Query Assistance** layer). As a user begins typing in the search bar, the app offers  **auto-complete suggestions drawn from the ontology** . Unlike typical keyword suggestions, these might complete partial words to valid product types (e.g. “charg…” suggesting “phone charger”, “USB cable”, etc.) using prefix matching on ontology terms. The suggestion engine can also leverage embedding similarity for smart suggestions, but often simple prefix and ontology lookup suffices given the controlled vocabulary. This feature increases success rate by aligning user queries with terms the system understands. After a search is executed, the UI can additionally show **related item suggestions** (Phase 2 feature) – essentially “You searched for X, you might also look for Y” where Y are semantically related items. For instance, after searching “camera film”, the app might suggest “camera battery” or “photo printing” as follow-up queries. These are generated from the ontology (siblings or children of the searched term). This encourages users to explore tangential needs or complementary items, further utilizing the breadth of local businesses. All suggestion mechanisms are powered by the ontology and semantic relationships established in Phase 1, demonstrating how the governed semantic data can enhance UX.

 In sum, the Buycott frontend offers a  **capability-centric discovery experience** . The design avoids traditional e-commerce patterns (no sorted lists by “best match” or user ratings) and instead visualizes each business in context (map location) with factual indicators. Features like capability lists, highlights, and suggestions turn raw semantic data into intuitive cues that invite users to discover and trust local businesses. All these UX decisions tie back to Phase 0/1 ideals: emphasize *immediate availability* (minutes-away), *semantic confidence* (evidence score), and *civic values* (local-first, transparency), while making the interface easy and even enjoyable to explore.

## 8. Civic-First Local Filtering and Accessibility

**Local-First Default Filtering:** A cornerstone of Buycott’s design is the emphasis on independent, local businesses. As an architectural rule from Phase 0, the search system by default **excludes chain stores** from results, unless the user opts-in to see them. This “local-first” filter is implemented at the query processing level (see Section 4) by automatically applying `is_chain = false` to every search query’s criteria. In Phase 2’s UI, this became an explicit toggle – e.g. a “Local only (on/off)” switch in the search interface that is ON by default. If a user toggles it off (or taps “Show chain stores”), the query is re-run without the chain filter so that larger retailers appear. The design rationale is to **prioritize discovery of independent businesses** in line with the civic mission, while still preserving user autonomy to view all options. Technically, this required classifying each business as chain or not. The data ingestion pipeline uses heuristics (like matching against known chain names or number of locations) to set the `is_chain` field, and Phase 2 added a dedicated field for chain name if applicable. The chain filter does not affect ranking (it’s a hard filter, not a weight), and it doesn’t hide information from users (since it’s user-controllable). It simply nudges the default experience toward local, civic outcomes as a form of embedded policy.

 **Discovery and Trust Considerations:** The local-first approach is complemented by UI cues that we discussed in Section 7 (like the independent badge and discovery highlighting for independent businesses). These reinforce to the user that the system values local independent establishments. Also, because the filter is transparent, users understand why they might not see a big-box store in results until they toggle “show chains”. This transparency and control build trust, avoiding any perception of bias – it’s clear the system has a pro-local stance by design, not due to hidden algorithms. The architecture supports this by making `include_chains` an explicit API parameter (default false), again reflecting Phase 0’s governance of even the API interface to encode neutrality and user choice.

 **Physical Accessibility Filters:** Phase 2 introduced additional filters to address the *immediate accessibility* of results, aligning with the principle of “minutes away vs days away.” One such feature is the  **Walking Distance filter** . When enabled (via a “Within walking distance” toggle in the UI), the system will restrict results to those within a certain travel time threshold by foot. For instance, it might filter to businesses reachable in 15 minutes or less walking. Implementing this required integrating a routing/distance service (e.g. Google or Mapbox API) into the backend: for each candidate result, the travel time from the user’s location is computed, and only those under the threshold are kept. The threshold could be user-adjustable in the future, but Phase 2 likely set a reasonable default (~15 min). This filter appeals to users who need something **immediately** nearby and furthers Buycott’s advantage over online shopping (you can get it now by walking a few blocks). It’s an entirely optional filter – by default, results are not limited to walking distance (they could be a short drive away), but users with limited mobility or who are on foot can activate it. In the API, this corresponds to a parameter like `walking_distance=true` and involves precomputing or dynamically calculating distances (caching route times for efficiency might be an implementation detail).

 Another key filter is  **Open-Now** . This allows users to see only businesses that are currently open at the time of search. The UI toggle for “Open now” triggers the backend to compare each candidate’s hours of operation against the current timestamp and include only those where the store is open. To support this, the system needed to store structured business hours (e.g., weekly schedule) in the database and have logic to determine openness. Phase 2 likely added a `business_hours` dataset (possibly as a JSON field or separate table) and an API endpoint or query filter to handle this. This feature greatly improves real-world usability – there’s no point sending a user to a shop that is closed, even if it semantically matches their query.

 Both “walking distance” and “open now” filters are **accessibility** features in a broad sense: they make the results more immediately actionable for the user’s situation (geographic and temporal accessibility). They also reinforce Buycott’s focus on immediacy and local convenience. From an architecture perspective, these filters are implemented as additional optional query constraints and simple UI toggles. They do not interfere with the core semantic ranking (which remains neutral); they only narrow the result set based on objective criteria. Phase 0’s API guidelines anticipated such filters by allowing parameters like `open_now` and `walking_distance` and explicitly stating they control filtering only. Thus, the introduction of these features in Phase 2 fit neatly into the existing API design without structural changes, demonstrating the foresight of the initial governance constraints.

## 9. API Design and Constraints

**API Philosophy:** The Buycott API is designed to expose the system’s capabilities to the frontend (and potentially third-party clients) while strictly enforcing the neutrality and semantic focus defined in Phase 0. All API endpoints and parameters are deliberately constrained to prevent any form of external manipulation of ranking or bias injection. In practice, the main endpoint is a **search API** (e.g. `GET /search`) which accepts a handful of query parameters: the search `query` text, a `location` (lat/long or region) to search around, and boolean flags corresponding to the user-controlled filters (e.g. `include_chains`, `open_now`, `walking_distance`). These parameters strictly influence filtering and scope – for example, `include_chains=false` applies the independent-only filter, `open_now=true` filters by hours, etc., as described in Section 8. The API does **not** accept any parameter that could bias the result ranking. Phase 0 explicitly banned parameters like `rank_by`, `boost`, `promote`, or any sort of paid prioritization flags. There is no way for a client to request sorting by popularity or to elevate certain businesses, because such options simply do not exist in the API specification. This is an architectural protection: even if someone built an alternative client or tried to hack the query, the backend would ignore or reject such instructions, ensuring fairness.

 **Endpoints and Responses:** Aside from the core search endpoint, Phase 2 introduced several supplementary endpoints to support the new features. These include:

* `GET /search_suggestions`: Takes a partial query and returns a list of ontology-based suggestion terms to autocomplete or refine the query.
* `GET /business_capabilities?business_id=X`: Returns the capability profile (ontology terms with confidence) for a given business, used for the Capability Discovery View.
* `GET /evidence_explanation?business_id=X&query=Y`: Returns the detailed evidence breakdown for why business X matched query Y. This might internally use the semantic match metadata computed during the search.
* `GET /filter_local_only` (or a parameter in search): Toggles the inclusion of chain stores. In implementation, this is likely just part of the main search endpoint (e.g. the `include_chains` param), but the design doc listed it for clarity.
* Similarly, `GET /filter_open_now` and `GET /filter_walking_distance` were listed, but those are effectively covered by parameters or combined calls. Possibly, they might also be implemented as separate endpoints that return a pre-filtered list given an existing result set. However, it’s more logical that these are integrated into `/search` as query params.

All responses from these endpoints are JSON structured data without any hidden ordering logic. For example, the `/search` response likely returns an array of result objects each containing fields like `business_id`, `name`, `location`, `distance`, `evidence_score`, etc., along with flags and maybe a snippet of text for context. The ordering of this array is by distance (closest first) by default, since that’s the only allowed sort besides perhaps alphabetical as a fallback. The API explicitly does not include any scoring in the sort and does not attach any “rank number” or “rating” in the payload. If an evidence score is included, it is there for display purposes and the consumer (frontend) knows not to treat it as a ranking key (the contract established in Phase 0 ensures evidence is for transparency only).

 From a technology perspective, the API is implemented using **FastAPI (Python)** on the backend. FastAPI was chosen in Phase 1 for its performance, easy asynchronous support (useful when calling external APIs like for distance matrix or performing vector searches), and concise declaration of endpoints that fit well with Python’s ML stack. The API layer handles incoming requests by orchestrating the pipeline: e.g. on a search request, it will call the embedding model to vectorize the query, query the Postgres/pgvector for nearest neighbors with the given filters, and stream back the results. The stateless nature of the API (no session-specific ranking) makes it easy to scale horizontally – multiple FastAPI workers can serve queries concurrently, all pulling from the same database. Caching layers can be introduced for repeated queries or suggestions if needed.

 **Security and Governance:** In keeping with governance, the API does not expose any debug or admin functions that could alter the ontology or database in ways violating the rules. Any update endpoints (if they exist for internal use) would enforce the same constraints (e.g. no adding forbidden fields). The public API is essentially read-only from the perspective of search data (all writes come from the extraction pipeline, not users or external agents). This minimizes the risk of spam or data poisoning. Also, by being open about the available endpoints and parameters (and their limitations), Buycott ensures third-party developers or auditors can verify that it’s not providing any hidden “backdoor” to promote certain results. The constraints defined in Phase 0 (allowed vs forbidden params, sorting rules) act as a contract for all future development on the API – even as features were added in Phase 2, they conformed to this blueprint.

## 10. Frontend Architecture (Flutter UI Components)

The Buycott frontend is implemented in  **Flutter** , allowing a single codebase to target mobile (iOS, Android) and web platforms. Flutter was selected in Phase 1 for its expressive UI capabilities (needed for the map and custom pins) and its cross-platform consistency, which aligns with the project’s resource efficiency (civic projects benefit from maintaining one codebase). The app’s architecture follows a layered UI approach with a permanent map layer and dynamic overlays:

* **Map Screen Base:** The main screen is a full-screen interactive map (using Google Maps or similar plugin for Flutter). This is always visible in the background, providing geographic context continuously. The user’s location is centered by default (with permission), and the map may auto-pan or zoom to fit search results as needed. This map layer is managed by a Flutter `MapWidget` and controllers for adding markers dynamically.
* **Primary Overlays:** On top of the map, key UI components are rendered as overlays. These include the **Search Bar** at the top – where users enter queries and see suggestions dropdown, and the **Floating Action Filters** or toggles (such as “Local only”, “Open now”) typically placed at an easily reachable position (e.g. above the bottom navigation, or as part of a filter drawer). Additionally, the search results **Pins/Markers** are overlayed on the map at their geolocation coordinates. Each pin widget includes the colored evidence/eta square as described earlier. Flutter’s widget flexibility allows these markers to be custom drawn (e.g. an icon or pin shape with a child widget showing the numbers).
* **Bottom Sheet & Panels:** When a user selects a pin, a **Bottom Sheet** slides up from the bottom containing detailed business info. This is typically a Flutter `DraggableScrollableSheet` or similar. It shows the business name, distance (“X minutes away”), evidence score (possibly as an icon or label), and action buttons (directions, call, etc.). It also includes tabs or expandable sections for things like the **Capability Profile** (“Likely carries...”), **Evidence Explanation** (if the user taps the evidence score, this could either navigate to a new screen or expand a section with the bullet points), and **Source Info** (“Sources: …”). In Phase 2, the design included an **Evidence Explanation panel** and a **Capability display panel** as new UI components. These can be implemented as either separate dialog screens or as part of the bottom sheet with internal navigation. Flutter’s navigation and state management should ensure that these panels can appear smoothly on top of the map context and be dismissed easily to return to the map.
* **Filter Toggles & Badges:** The UI includes several toggle controls for filters: e.g. a Local-only toggle (default on) and a “Show Chains” toggle (could be a single switch that flips state), as well as the Open Now and Walking Distance toggles. In Flutter, these might be implemented in an AppBar action or as part of a filter menu accessible from the main screen. Given the emphasis, they might be always visible switches for quick access. The **badges** (Specialist, Independent) appear likely in the bottom sheet or perhaps on the pins themselves as a small icon. For example, an independent business could have a small “indie” icon on its pin or next to its name in the sheet. A specialist badge might show in the detail view to indicate this business has a narrow focus. These badges are simple UI elements conditioned on data (like `is_chain` false triggers showing the Independent badge).
* **State Management:** While not explicitly detailed in the docs, the app likely uses a state management approach (like Provider or BLoC pattern) to handle the search state and map state. For instance, when a search query is submitted, the app calls the backend API (via HTTP) and then places pins on the map for each result. The state includes the current list of results, the applied filters, and the selected business for the detail view. Flutter’s reactive UI will rebuild the widgets (pins, list, bottom sheet) when this state updates. The architecture ensures a responsive experience – e.g., toggling “Open now” immediately filters the in-memory results or triggers a new API call, and the UI updates accordingly.

**User Experience Focus:** The frontend architecture is driven by ease of use and clarity. By using a familiar map interface with minimal clutter, it lowers the learning curve – users instinctively pan the map, tap pins, etc. The structured overlays (search bar, suggestions dropdown, results sheet) guide users through the discovery process. Even the suggestion dropdown is semantically powered, as described, and integrated into the search bar component. Flutter allows custom styling to maintain a coherent visual language (civic, trustworthy aesthetics). Performance is also critical – Flutter’s rendering ensures smooth animations (e.g. pin drops, sheet transitions) at 60fps, and the static map avoids constant re-renders. Testing in Phase 1/2 included verifying that adding ~50 pins with complex widgets is still performant on typical devices.

 Overall, the choice of Flutter and the UI component architecture achieved a few key Phase 1 goals: a **transit-style, map-first UX** that always orients by location, **immediate utility** features like showing travel minutes prominently, and **transparency** via evidence indicators. Phase 2’s additions (toggles, explanation panels, suggestions) were integrated without altering the fundamental structure – they appear as additional widgets and calls, keeping the separation of concerns (map vs. overlays vs. panels) intact. This modular UI approach in Flutter means new features can be added as components without rewiring the whole app.

## 11. Backend Stack and Infrastructure

**Tech Stack Overview:** Buycott’s backend is composed of a modern, open-source stack chosen for alignment with the project’s needs and values. The **database layer** is PostgreSQL paired with the `pgvector` extension for vector similarity search. This serves as both the system of record for business data (relational storage) and the engine for semantic nearest-neighbor queries on embeddings. The **API layer** is built with Python and FastAPI, offering an asynchronous web framework that easily ties into Python’s data science ecosystem (for calling embedding models, etc.). The API layer contains the implementation of the search pipeline logic (embedding generation, ontology lookup, DB queries) and the various endpoints described earlier. FastAPI also auto-documents these endpoints, fitting the transparency ethos.

 **Embedding & ML Infrastructure:** For generating embeddings and other ML tasks (ontology similarity, etc.), the backend uses Python ML libraries (such as Hugging Face’s Transformers or SentenceTransformers) running on a suitable compute environment. Phase 1’s design mentions using an **NVIDIA GPU cluster with Spark** for the extraction/embedding pipeline. In practice, this likely means that offline jobs (like initial data processing or periodic reprocessing of all businesses) can be distributed across a cluster to utilize GPU acceleration. Spark’s mention suggests they might use PySpark or a similar distributed data framework to coordinate the crawling and embedding tasks, especially as the number of businesses grows. However, for real-time query handling, the FastAPI service itself could use a smaller model on CPU or a single GPU to embed user queries quickly (the suggested MiniLM model is efficient enough for sub-100ms encoding on CPU, and can be loaded in memory). The architecture thus may have two modes: **offline batch processing** (via Spark jobs or cron-triggered tasks that run OpenClaw and update the DB) and **online query serving** (FastAPI with possibly an in-memory model for query embedding).

 **Integration and Infrastructure:** All components communicate over well-defined interfaces:

* OpenClaw agents write to the database (through an ORM or directly via SQL) the new/updated business records, their embeddings, and source/capability entries.
* The FastAPI server reads from the same database for answering queries. Because the data changes relatively slowly (new businesses or updated info), heavy caching isn’t required beyond what Postgres and OS already cache in memory.
* For third-party services: the backend calls external APIs for certain features – e.g., a routing API for travel time (used in computing “minutes away” and filtering walking distance), or possibly geocoding if freeform locations are accepted. These calls are made on the fly during requests or precomputed if possible (e.g., travel times to all results might be fetched in parallel after initial vector search).
* Everything is containerized for deployment (implied best practice): e.g. a Docker container for the FastAPI app, one for a worker that runs extraction jobs, and a Postgres container. This was not explicitly stated but is a reasonable assumption.

**Scalability and Hosting:** The system is initially deployed as an MVP in Phase 1 with modest scale (a single Postgres instance, a couple of API servers). Because of the clean separation of concerns, it scales horizontally: we can add more API servers behind a load balancer if query volume grows, scale up the Postgres or use read replicas if needed for heavy read load, and distribute crawling agents across multiple machines. The use of proven tech (Postgres, Spark, FastAPI) means we can rely on existing tools for monitoring, replication, etc. Also, since the entire codebase is open-source, it can be deployed by civic tech communities in different regions, requiring only configuration of the data sources and API keys (for maps/routing).

 **Backend Constraints:** The infrastructure choices were guided by Phase 0’s permanent constraints. For example, using an open-source database and AI models aligns with the **civic alignment** (no dependency on closed commercial services that could introduce bias or cost issues). The entire stack is privacy-conscious: no component collects personal user data; queries are ephemeral and not tied to user identity (unless analytics are added with consent). The result is an architecture that is robust, explainable, and easy to maintain by a small team, which was crucial for Phase 1’s implementation timeline. Phase 2 confirmed that these stack choices were sufficient – new features like additional endpoints and filters were added without needing to change the database technology or the web framework, validating the flexibility of the initial stack. For instance, adding the `business_capabilities` table and extra endpoints was straightforward in Postgres and FastAPI. The performance remained acceptable through Phase 2, as vector queries and small ontology lookups are well within Postgres’ capability for the expected data size (a city’s worth of businesses, maybe tens of thousands of vectors).

 In summary, the backend is a **Python/Postgres-based semantic search service** augmented by a crawling/ML pipeline. Its infrastructure is chosen to enforce openness (all components are inspectable) and to allow continuous improvement (e.g., swapping out the embedding model or adding new data fields doesn’t require a complete redesign). This solid foundation from Phase 1 carried through Phase 2 with minimal changes, demonstrating architectural stability.

## 12. Development Slices and Validation Workflow

From the very beginning, Buycott adopted a **Vertical Slice development model** (established in Phase 0) to ensure that each increment of the system is functional end-to-end and stays within the architectural guardrails. A vertical slice means a feature is implemented across all layers of the stack in a coherent increment. For example, in Phase 1’s MVP, the team delivered a basic semantic search slice: this included the extraction layer (some initial data and embeddings), the ontology setup, the vector search query, and the UI map view showing results with evidence. By not developing any layer in isolation, the team could immediately validate the real-world behavior and neutrality of the system.

 **Vertical Slice Composition:** Every feature or user story is tackled by touching the necessary backend components, data model, and frontend elements in one iteration. According to Phase 0 guidelines, a proper slice includes:  *data extraction, embedding generation, ontology expansion, vector search, UI visualization, and evidence display* . This comprehensive approach prevents architectural “shortcuts” – for instance, one cannot implement a UI feature that requires a new type of data without also adding that data to the schema and extraction process in the same slice. It also means each feature is immediately testable in terms of the core mission: does the semantic matching actually work for that feature? Does the evidence remain explainable?

 **Semantic Validation:** A critical part of the workflow is validating each vertical slice against the semantic and civic criteria. Phase 0 required that with each new slice, the team must verify: (a) the semantic matching correctness (i.e. are the right businesses coming up for the right reasons?), (b) ontology expansion correctness (are the ontology relationships helping, and not causing false matches?), and (c) evidence explainability (can we explain the results in a way users understand?). In practice, this means writing tests or conducting user testing for each feature. For instance, when the “Open Now filter” was added in Phase 2, the team would validate that searching with it on indeed excludes closed stores and that this matches user expectation and doesn’t inadvertently hide needed results. Similarly, when introducing “related item suggestions,” they likely checked that the suggestions truly made sense via the ontology (no random or out-of-scope suggestions).

 **Continuous Ontology Refinement:** Because the ontology is central and evolving, the development workflow includes refining the ontology as slices are built. If during a slice validation the team finds a semantic gap (e.g. the system didn’t catch a synonym or a category needed adjustment), they update the ontology accordingly. This is done carefully under the governance rules (no violating structure or neutrality). Thus, development is also iterative on the knowledge model itself, not just code. The *vertical slice + validation* approach catches these needs early, rather than after a full system is built.

 **Phase-based Delivery:** Phases 1 and 2 can themselves be seen as larger slices or collections of slices:

* **Phase 1 (MVP)** delivered the core vertical slice of functionality: basic business extraction, embedding, semantic search (L1–L4 queries), the map UI with evidence scores, and distance computation. This was the foundation and was validated to meet the mission goals in a limited form.
* **Phase 2** then consisted of several smaller feature slices built on top: ontology integration (as a slice adding the ontology DB and expansion engine), then evidence explanation slice, discovery UX slice (suggestions, highlights), and filter slice (local toggle, open/walk filters). Each of these slices was integrated one by one, each meeting the criteria above. Because of the strong Phase 0 constraints, each new feature had to “plug in” without breaking rules. The result, as noted in the Phase 2 critique, was *minimal architectural disruption* – Phase 2’s enhancements were achieved by extending the existing patterns rather than refactoring. The vertical slice method ensured that as each capability was added, the system remained coherent and stable, and any issues were surfaced immediately in that slice’s context.

**Testing and Iteration:** The development workflow emphasizes testing at the slice level. For example, after implementing the Evidence Explanation feature, the team would test a few query scenarios end-to-end: does the backend correctly store the needed evidence data and does the frontend panel show the right info for a given search result? Similarly, for the walking distance filter, they would test with known distances. Automated tests likely cover the pipeline (unit tests for ontology expansion logic, integration tests for the API filtering, etc.) and scenario tests using sample data. User testing is also critical given the civic focus – getting feedback on whether the explanations make sense, whether the UI is clear, etc., feeding into quick adjustments in the slice.

 **Governance Enforcement:** At each code review or feature design, the team cross-checks Phase 0’s invariant list (system identity, forbidden fields, allowed behaviors, etc.). This acts as a checklist to ensure no one accidentally introduces a disallowed element. For example, if a developer suggested adding a “user rating” field to the business for some reason, the governance layer would flag this as against the rules (engagement metric, not allowed). Likewise, if a slice required an API change, they would ensure no forbidden param is introduced. This discipline is part of the team culture set in Phase 0.

 In conclusion, the vertical slice and validation-driven development methodology has been instrumental in Buycott’s architecture. It guaranteed that  **each phase’s contributions are integrated, testable, and aligned with core principles** . By Phase 2, this approach proved its value: the system grew in capabilities (from a simple semantic search to a rich discovery platform with explanations and filters) without losing its original identity or stability. Future development (Phase 3 and beyond) will continue with this model, ensuring that Buycott scales and adapts while remaining *“a transparent civic semantic discovery infrastructure”* true to its Phase 0 blueprint.
